{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIT205 Thinking Systems and Cognition Science - Assignment 2\n",
    "\n",
    "## Group: Philip Castiglione (217157862) and Warwick Smith (215239649)\n",
    "\n",
    "## Topic 1: Text Analysis\n",
    "\n",
    "## Project Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introductory comments regarding theme of analysis, data source used (e.g. Twitter), etc.\n",
    "\n",
    "Note, we use local caching of results using files on disk to reduce api calls and allow us to run different parts of this notebook more independently\n",
    "\n",
    "Note for report\n",
    "we investigated #libspill, #auspol and #MalcolmTurnbull. #libspill produced the most interesting results for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- cleanup\n",
    "- run with a big target\n",
    "- markdown throughout this file\n",
    "- collect libspill tweets around 21st august ish, should be more dramatic\n",
    "- final report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Document Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "import statistics\n",
    "\n",
    "# Additional libraries\n",
    "import twitter                    # python-twitter API\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 import *\n",
    "from dotenv import load_dotenv    # for management of twitter credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `dotenv` library to load key-value pairs from a `.env` file into the os environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilise external environment to source our API authentication credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_CONSUMER_KEY = os.getenv(\"TWITTER_CONSUMER_KEY\")\n",
    "TWITTER_CONSUMER_SECRET = os.getenv(\"TWITTER_CONSUMER_SECRET\")\n",
    "TWITTER_ACCESS_TOKEN_KEY = os.getenv(\"TWITTER_ACCESS_TOKEN_KEY\")\n",
    "TWITTER_ACCESS_TOKEN_SECRET = os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n",
    "WATSON_NLU_API_KEY = os.getenv(\"WATSON_NLU_API_KEY\")\n",
    "\n",
    "# WARNING: do not commit to git with any of these values printed to a cell's output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create python-twitter API instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = twitter.Api(consumer_key=TWITTER_CONSUMER_KEY,\n",
    "                  consumer_secret=TWITTER_CONSUMER_SECRET,\n",
    "                  access_token_key=TWITTER_ACCESS_TOKEN_KEY,\n",
    "                  access_token_secret=TWITTER_ACCESS_TOKEN_SECRET,\n",
    "                  tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter hashtag for searching to find documents for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = \"libspill\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for tweet collection\n",
    "def collect_tweets(api, hashtag, batch_max, total_count):\n",
    "    \"\"\"\n",
    "    Function to collects tweets using the python-twitter GetSearch API\n",
    "    \n",
    "    api:         Twitter API instance\n",
    "    hashtag:     search hashtag\n",
    "    batch_max:   maximum number of tweets to collect per each request\n",
    "    total_count: maximum number of tweets to collect in total\n",
    "    \"\"\"\n",
    "    \n",
    "    # the collection of tweets to be returned\n",
    "    tweets = []\n",
    "    batch_max = str(batch_max)\n",
    "    \n",
    "    #collect the first batch of tweets\n",
    "\n",
    "    results = api.GetSearch(term=hashtag, result_type=\"recent\", lang=\"en\", \n",
    "                            count=batch_max, return_json=True)\n",
    "    \n",
    "    #add results to list\n",
    "    tweets += results['statuses']\n",
    "    \n",
    "    # find the the relevant starting ID for the next search\n",
    "    ids = [tweet['id'] for tweet in tweets]\n",
    "    max_tweet_id = str(min(ids)-1)   #reduced the minimum ID by 1 to remove duplication of tweets at the start/end of batches.\n",
    "\n",
    "    # collect the remaining batches in the total_count\n",
    "    while len(tweets) < total_count:\n",
    "        \n",
    "        print(\"{} tweets collected for hashtag {}. Most recent tweeted at {}\".format(\n",
    "            len(tweets), hashtag, tweets[len(tweets)-1]['created_at']))\n",
    "        \n",
    "        #add tweet_mode=extended to the below\n",
    "        results = api.GetSearch(term=hashtag, result_type=\"recent\", lang=\"en\", \n",
    "                                count=batch_max, return_json=True, max_id=max_tweet_id)\n",
    "        tweets += results['statuses']\n",
    "        ids = [tweet['id'] for tweet in tweets]\n",
    "        max_tweet_id = str(min(ids)-1)   #reduced the minimum ID by 1 to remove duplication of tweets at the start/end of batches.\n",
    "        \n",
    "    print(\"{} tweets collected for hashtag {}. Most recent tweeted at {}\".format(\n",
    "            len(tweets), hashtag, tweets[len(tweets)-1]['created_at']))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tweets collected for hashtag libspill. Most recent tweeted at Fri Sep 14 17:57:21 +0000 2018\n"
     ]
    }
   ],
   "source": [
    "tweet_collection = collect_tweets(api, hashtag, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filename = f\"cached_tweets_{hashtag}.pkl\"\n",
    "with open(tweets_filename, 'wb') as f:\n",
    "    pickle.dump(tweet_collection, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Document Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_collection = None\n",
    "with open(tweets_filename, 'rb') as f:\n",
    "    tweet_collection = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a series of transformations in a pipeline to extract documents in the format and with the content we want, from the tweet collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_documents(tweets):\n",
    "    extract_text = lambda tweets: [tweet['full_text'] for tweet in tweets]\n",
    "    convert_whitespace_chars = lambda tweets: [tweet.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ') for tweet in tweets]\n",
    "    squash_whitespace = lambda tweets: [tweet.replace('  ', ' ') for tweet in tweets]\n",
    "    tokenize = lambda tweets: [tweet.strip().split() for tweet in tweets]\n",
    "    strip_links = lambda tweets: [[token for token in tokens if \"http\" not in token] for tokens in tweets]\n",
    "    strip_mentions = lambda tweets: [[token for token in tokens if token[0] is not '@'] for tokens in tweets]\n",
    "    strip_hashtags = lambda tweets: [[token for token in tokens if token[0] is not '#'] for tokens in tweets]\n",
    "    filter_empty = lambda tweets: [tweet for tweet in tweets if len(tweet) > 0]\n",
    "    filter_retweets = lambda tweets: [tweet for tweet in tweets if tweet[0] != 'RT']\n",
    "    rejoin = lambda tweets: [' '.join(tokens) for tokens in tweets]\n",
    "\n",
    "    documents = tweets\n",
    "    \n",
    "    for transformation in [\n",
    "        extract_text,\n",
    "        convert_whitespace_chars,\n",
    "        squash_whitespace,\n",
    "        tokenize,\n",
    "        strip_links,\n",
    "        strip_mentions,\n",
    "        strip_hashtags,\n",
    "        filter_empty,\n",
    "        filter_retweets,\n",
    "        filter_empty,\n",
    "        rejoin,\n",
    "    ]:\n",
    "        documents = transformation(documents)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = extract_documents(tweet_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Watson NLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an nlu instance using ibm watson sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://gateway-syd.watsonplatform.net/natural-language-understanding/api\"\n",
    "version = \"2018-03-19\"\n",
    "\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version=version,\n",
    "    iam_apikey=WATSON_NLU_API_KEY,\n",
    "    url=url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve analyses for each of our documents using the nlu instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analyses(documents):\n",
    "    entities = EntitiesOptions(sentiment=True, emotion=True, limit=5)\n",
    "    sentiment = SentimentOptions()\n",
    "    categories = CategoriesOptions()\n",
    "    keywords = KeywordsOptions(sentiment=True, emotion=True, limit=5)\n",
    "    emotion = EmotionOptions()\n",
    "    \n",
    "    def analyze(document):\n",
    "        return natural_language_understanding.analyze(\n",
    "            text=document,\n",
    "            features=Features(\n",
    "                entities=entities,\n",
    "                sentiment=sentiment,\n",
    "                categories=categories,\n",
    "                keywords=keywords,\n",
    "                emotion=emotion,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return [analyze(document) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "watson_analyses = get_analyses(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out analyses to local disk, so we can perform analysis without refetching each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "watson_analysis_filename = f\"cached_watson_analysis_{hashtags[0]}.pkl\"\n",
    "\n",
    "with open(watson_analysis_filename, 'wb') as f:\n",
    "    pickle.dump(watson_analyses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Analysis of Watson Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in our Watson analyses from disk, build five analysis objects with the information we need for our report, and write those objects out to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "watson_analyses = None\n",
    "with open(watson_analysis_filename, 'rb') as f:\n",
    "    watson_analyses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_anaylsis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_labels = [analysis.result['sentiment']['document']['label'] for analysis in analyses]\n",
    "sentiment_counter = Counter(sentiment_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_anaylsis['positive_percentage'] = 100 * sentiment_counter['positive'] / sum(sentiment_counter.values())\n",
    "sentiment_anaylsis['neutral_percentage'] = 100 * sentiment_counter['neutral'] / sum(sentiment_counter.values())\n",
    "sentiment_anaylsis['negative_percentage'] = 100 * sentiment_counter['negative'] / sum(sentiment_counter.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = [analysis.result['sentiment']['document']['score'] for analysis in analyses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_anaylsis['average_score'] = sum(sentiment_scores) / len(sentiment_scores)\n",
    "sentiment_anaylsis['score_std_dev'] = statistics.stdev(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive_percentage': 8.333333333333334,\n",
       " 'neutral_percentage': 19.444444444444443,\n",
       " 'negative_percentage': 72.22222222222223,\n",
       " 'average_score': -0.402114913888889,\n",
       " 'score_std_dev': 0.4068022533357083}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_anaylsis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion  Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_analysis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document level emotion scores\n",
    "for emotion in ['sadness', 'joy', 'fear', 'disgust', 'anger']:\n",
    "    emotion_scores = [analysis.result['emotion']['document']['emotion'][emotion] for analysis in analyses]\n",
    "\n",
    "    average_emotion_score = sum(emotion_scores) / len(emotion_scores)\n",
    "    emotion_score_std_dev = statistics.stdev(emotion_scores)\n",
    "    emotion_analysis[emotion] = {'average_score': average_emotion_score, 'score_std_dev': emotion_score_std_dev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sadness': {'average_score': 0.2116943611111111,\n",
       "  'score_std_dev': 0.14032413936681493},\n",
       " 'joy': {'average_score': 0.20690461111111114,\n",
       "  'score_std_dev': 0.22862744060080148},\n",
       " 'fear': {'average_score': 0.12323252777777778,\n",
       "  'score_std_dev': 0.06747994023051215},\n",
       " 'disgust': {'average_score': 0.22668377777777776,\n",
       "  'score_std_dev': 0.22556472631307417},\n",
       " 'anger': {'average_score': 0.23425372222222224,\n",
       "  'score_std_dev': 0.19718658102063222}}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_analysis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses_categories = [analysis.result['categories'] for analysis in analyses]\n",
    "labels = lambda categories: [category['label'] for category in categories]\n",
    "category_labels = []\n",
    "[category_labels.extend(labels(categories)) for categories in analyses_categories]\n",
    "categeories_counter = Counter(category_labels)\n",
    "category_analysis['count'] = sum(categeories_counter.values())\n",
    "category_analysis['categories'] = dict(categeories_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_analysis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses_entities = [analysis.result['entities'] for analysis in analyses]\n",
    "raw_entities = lambda entities: [entity for entity in entities]\n",
    "entities = []\n",
    "[entities.extend(raw_entities(analyses_entity)) for analyses_entity in analyses_entities]\n",
    "\n",
    "unique_entity_names = set([entity['text'] for entity in entities])\n",
    "\n",
    "entity_analysis['count'] = len(unique_entity_names)\n",
    "entity_analysis['entities'] = {}\n",
    "\n",
    "for entity_name in unique_entity_names:\n",
    "    sentiments = []\n",
    "    for entity in entities:\n",
    "        if entity['text'] == entity_name:\n",
    "            sentiments.append(entity['sentiment']['score'])\n",
    "    entity_analysis['entities'][entity_name] = {}\n",
    "    entity_analysis['entities'][entity_name]['frequency'] = len(sentiments)\n",
    "    entity_analysis['entities'][entity_name]['average_sentiment'] = sum(sentiments) / len(sentiments)\n",
    "    if (len(sentiments) > 1):\n",
    "        entity_analysis['entities'][entity_name]['sentiment_std_dev'] = statistics.stdev(sentiments)\n",
    "    else:\n",
    "        entity_analysis['entities'][entity_name]['sentiment_std_dev'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_analysis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses_keywords = [analysis.result['keywords'] for analysis in analyses]\n",
    "raw_keywords = lambda keywords: [keyword for keyword in keywords]\n",
    "keywords = []\n",
    "[keywords.extend(raw_keywords(analyses_keyword)) for analyses_keyword in analyses_keywords]\n",
    "\n",
    "unique_keyword_names = set([keyword['text'] for keyword in keywords])\n",
    "\n",
    "keyword_analysis['count'] = len(unique_keyword_names)\n",
    "keyword_analysis['keywords'] = {}\n",
    "\n",
    "for keyword_name in unique_keyword_names:\n",
    "    sentiments = []\n",
    "    for keyword in keywords:\n",
    "        if keyword['text'] == keyword_name:\n",
    "            sentiments.append(keyword['sentiment']['score'])\n",
    "    keyword_analysis['keywords'][keyword_name] = {}\n",
    "    keyword_analysis['keywords'][keyword_name]['frequency'] = len(sentiments)\n",
    "    keyword_analysis['keywords'][keyword_name]['average_sentiment'] = sum(sentiments) / len(sentiments)\n",
    "    if (len(sentiments) > 1):\n",
    "        keyword_analysis['keywords'][keyword_name]['sentiment_std_dev'] = statistics.stdev(sentiments)\n",
    "    else:\n",
    "        keyword_analysis['keywords'][keyword_name]['sentiment_std_dev'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write our analyses out to disk, so they can be used in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_analyses = {\n",
    "    'sentiment': sentiment_anaylsis,\n",
    "    'emotion': emotion_analysis,\n",
    "    'category': category_analysis,\n",
    "    'keyword': keyword_analysis,\n",
    "    'entity': entity_analysis,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_analysis_filename = f\"cached_report_analysis_{hashtags[0]}.pkl\"\n",
    "\n",
    "with open(report_analysis_filename, 'wb') as f:\n",
    "    pickle.dump(report_analyses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
