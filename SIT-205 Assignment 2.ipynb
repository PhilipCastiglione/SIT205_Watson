{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIT205 Thinking Systems and Cognition Science - Assignment 2\n",
    "\n",
    "## Group: Philip Castiglione (217157862) and Warwick Smith (215239649)\n",
    "\n",
    "## Topic 1: Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to analyse the recent events of Australian politics using text produced by members of the general public on Twitter.\n",
    "\n",
    "To do this, we collected approximately 6,000 tweets containing the hashtag #libspill using the Twitter API. We obtain these using the open source `python-twitter` [client library](https://github.com/bear/python-twitter).\n",
    "\n",
    "We then clean and filter these tweets to leave us with a set of 941 unique documents for textual analysis.\n",
    "\n",
    "We send these documents to the Watson Natural Language Understanding API.\n",
    "\n",
    "Finally, we complete a statistical analysis of the results.\n",
    "\n",
    "This notebook is separated into 4 parts:\n",
    "\n",
    "1. [Part 1 - Document Collection](#Part-1---Document-Collection)\n",
    "1. [Part 2 - Document Cleaning](#Part-2---Document-Cleaning)\n",
    "1. [Part 3 - Watson NLU](#Part-3---Watson-NLU)\n",
    "1. [Part 4 - Analysis of Watson Output](#Part-4---Analysis-of-Watson-Output)\n",
    "\n",
    "Notes:\n",
    "- Each part writes out results to a file, to allow us to work on sections independently, and to save on API calls\n",
    "- We ran the analysis at Mon 17 Sept 2018 and the number of tweets and documents were from this run. Running at a different point in time will lead to different data capture and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "import statistics\n",
    "\n",
    "# Additional libraries\n",
    "import twitter                    # python-twitter API\n",
    "from watson_developer_cloud import NaturalLanguageUnderstandingV1\n",
    "from watson_developer_cloud.natural_language_understanding_v1 import *\n",
    "from dotenv import load_dotenv    # for management of twitter credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `dotenv` library to load key-value pairs from a `.env` file into the os environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilise external environment to source our API authentication credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWITTER_CONSUMER_KEY = os.getenv(\"TWITTER_CONSUMER_KEY\")\n",
    "TWITTER_CONSUMER_SECRET = os.getenv(\"TWITTER_CONSUMER_SECRET\")\n",
    "TWITTER_ACCESS_TOKEN_KEY = os.getenv(\"TWITTER_ACCESS_TOKEN_KEY\")\n",
    "TWITTER_ACCESS_TOKEN_SECRET = os.getenv(\"TWITTER_ACCESS_TOKEN_SECRET\")\n",
    "WATSON_NLU_API_KEY = os.getenv(\"WATSON_NLU_API_KEY\")\n",
    "\n",
    "# WARNING: do not commit to git with any of these values printed to a cell's output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Contents](#Topic-1:-Text-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create python-twitter API instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = twitter.Api(consumer_key=TWITTER_CONSUMER_KEY,\n",
    "                  consumer_secret=TWITTER_CONSUMER_SECRET,\n",
    "                  access_token_key=TWITTER_ACCESS_TOKEN_KEY,\n",
    "                  access_token_secret=TWITTER_ACCESS_TOKEN_SECRET,\n",
    "                  tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter search hashtag to find documents for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag = \"libspill\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect tweets in batches, up to a particular count, total_count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_tweets(api, hashtag, batch_max, total_count):\n",
    "    tweets = []\n",
    "    batch_max = str(batch_max)\n",
    "    results = api.GetSearch(term=hashtag, result_type=\"recent\", lang=\"en\", \n",
    "                            count=batch_max, return_json=True)\n",
    "\n",
    "    tweets += results['statuses']\n",
    "    \n",
    "    ids = [tweet['id'] for tweet in tweets]\n",
    "    max_tweet_id = str(min(ids)-1)\n",
    "    \n",
    "    previous_tweet_count = 0\n",
    "    while len(tweets) < total_count and len(tweets) > previous_tweet_count:\n",
    "        previous_tweet_count = len(tweets)\n",
    "        \n",
    "        print(\"{} tweets collected for hashtag {}. Most recent tweeted at {}\".format(\n",
    "            len(tweets), hashtag, tweets[len(tweets)-1]['created_at']))\n",
    "        \n",
    "        results = api.GetSearch(term=hashtag, result_type=\"recent\", lang=\"en\", \n",
    "                                count=batch_max, return_json=True, max_id=max_tweet_id)\n",
    "        tweets += results['statuses']\n",
    "        ids = [tweet['id'] for tweet in tweets]\n",
    "        max_tweet_id = str(min(ids)-1)\n",
    "        \n",
    "    print(\"{} tweets collected for hashtag {}. Most recent tweeted at {}\".format(\n",
    "            len(tweets), hashtag, tweets[len(tweets)-1]['created_at']))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow for tweet cleaning, and particularly filtering of re-tweets, we aim to collect 6,000 tweets relating to the #libspill hashtag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 17 05:59:07 +0000 2018\n",
      "200 tweets collected for hashtag libspill. Most recent tweeted at Sun Sep 16 23:05:12 +0000 2018\n",
      "300 tweets collected for hashtag libspill. Most recent tweeted at Sun Sep 16 08:46:30 +0000 2018\n",
      "400 tweets collected for hashtag libspill. Most recent tweeted at Sun Sep 16 00:06:21 +0000 2018\n",
      "500 tweets collected for hashtag libspill. Most recent tweeted at Sat Sep 15 22:57:07 +0000 2018\n",
      "600 tweets collected for hashtag libspill. Most recent tweeted at Sat Sep 15 08:51:00 +0000 2018\n",
      "700 tweets collected for hashtag libspill. Most recent tweeted at Sat Sep 15 01:24:29 +0000 2018\n",
      "800 tweets collected for hashtag libspill. Most recent tweeted at Fri Sep 14 13:27:57 +0000 2018\n",
      "900 tweets collected for hashtag libspill. Most recent tweeted at Fri Sep 14 07:47:36 +0000 2018\n",
      "1000 tweets collected for hashtag libspill. Most recent tweeted at Fri Sep 14 04:19:05 +0000 2018\n",
      "1100 tweets collected for hashtag libspill. Most recent tweeted at Fri Sep 14 02:44:54 +0000 2018\n",
      "1200 tweets collected for hashtag libspill. Most recent tweeted at Fri Sep 14 01:08:42 +0000 2018\n",
      "1300 tweets collected for hashtag libspill. Most recent tweeted at Thu Sep 13 22:08:58 +0000 2018\n",
      "1400 tweets collected for hashtag libspill. Most recent tweeted at Thu Sep 13 20:18:51 +0000 2018\n",
      "1500 tweets collected for hashtag libspill. Most recent tweeted at Thu Sep 13 13:50:57 +0000 2018\n",
      "1583 tweets collected for hashtag libspill. Most recent tweeted at Thu Sep 13 10:54:01 +0000 2018\n",
      "1683 tweets collected for hashtag libspill. Most recent tweeted at Thu Sep 13 08:05:47 +0000 2018\n",
      "1783 tweets collected for hashtag libspill. Most recent tweeted at Thu Sep 13 06:52:24 +0000 2018\n",
      "1883 tweets collected for hashtag libspill. Most recent tweeted at Thu Sep 13 05:27:27 +0000 2018\n",
      "1983 tweets collected for hashtag libspill. Most recent tweeted at Thu Sep 13 04:30:59 +0000 2018\n",
      "2083 tweets collected for hashtag libspill. Most recent tweeted at Thu Sep 13 03:00:19 +0000 2018\n",
      "2183 tweets collected for hashtag libspill. Most recent tweeted at Thu Sep 13 01:24:21 +0000 2018\n",
      "2283 tweets collected for hashtag libspill. Most recent tweeted at Wed Sep 12 23:34:33 +0000 2018\n",
      "2383 tweets collected for hashtag libspill. Most recent tweeted at Wed Sep 12 21:42:06 +0000 2018\n",
      "2483 tweets collected for hashtag libspill. Most recent tweeted at Wed Sep 12 15:37:48 +0000 2018\n",
      "2583 tweets collected for hashtag libspill. Most recent tweeted at Wed Sep 12 12:23:37 +0000 2018\n",
      "2683 tweets collected for hashtag libspill. Most recent tweeted at Wed Sep 12 10:04:18 +0000 2018\n",
      "2783 tweets collected for hashtag libspill. Most recent tweeted at Wed Sep 12 07:58:29 +0000 2018\n",
      "2883 tweets collected for hashtag libspill. Most recent tweeted at Wed Sep 12 06:14:29 +0000 2018\n",
      "2983 tweets collected for hashtag libspill. Most recent tweeted at Wed Sep 12 04:42:39 +0000 2018\n",
      "3083 tweets collected for hashtag libspill. Most recent tweeted at Wed Sep 12 03:40:28 +0000 2018\n",
      "3183 tweets collected for hashtag libspill. Most recent tweeted at Wed Sep 12 02:56:04 +0000 2018\n",
      "3283 tweets collected for hashtag libspill. Most recent tweeted at Wed Sep 12 00:54:13 +0000 2018\n",
      "3383 tweets collected for hashtag libspill. Most recent tweeted at Tue Sep 11 22:26:15 +0000 2018\n",
      "3483 tweets collected for hashtag libspill. Most recent tweeted at Tue Sep 11 14:37:16 +0000 2018\n",
      "3583 tweets collected for hashtag libspill. Most recent tweeted at Tue Sep 11 10:37:20 +0000 2018\n",
      "3683 tweets collected for hashtag libspill. Most recent tweeted at Tue Sep 11 08:26:52 +0000 2018\n",
      "3783 tweets collected for hashtag libspill. Most recent tweeted at Tue Sep 11 06:10:00 +0000 2018\n",
      "3883 tweets collected for hashtag libspill. Most recent tweeted at Tue Sep 11 04:36:11 +0000 2018\n",
      "3983 tweets collected for hashtag libspill. Most recent tweeted at Tue Sep 11 03:43:34 +0000 2018\n",
      "4083 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 10 22:45:35 +0000 2018\n",
      "4183 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 10 14:38:39 +0000 2018\n",
      "4283 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 10 11:21:47 +0000 2018\n",
      "4383 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 10 08:27:23 +0000 2018\n",
      "4483 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 10 06:55:23 +0000 2018\n",
      "4583 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 10 05:47:20 +0000 2018\n",
      "4683 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 10 04:51:46 +0000 2018\n",
      "4783 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 10 03:56:28 +0000 2018\n",
      "4878 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 10 03:00:07 +0000 2018\n",
      "4978 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 10 01:57:18 +0000 2018\n",
      "5078 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 10 01:06:35 +0000 2018\n",
      "5178 tweets collected for hashtag libspill. Most recent tweeted at Mon Sep 10 00:25:21 +0000 2018\n",
      "5278 tweets collected for hashtag libspill. Most recent tweeted at Sun Sep 09 23:40:11 +0000 2018\n",
      "5378 tweets collected for hashtag libspill. Most recent tweeted at Sun Sep 09 22:29:16 +0000 2018\n",
      "5478 tweets collected for hashtag libspill. Most recent tweeted at Sun Sep 09 21:23:56 +0000 2018\n",
      "5578 tweets collected for hashtag libspill. Most recent tweeted at Sun Sep 09 15:04:34 +0000 2018\n",
      "5678 tweets collected for hashtag libspill. Most recent tweeted at Sun Sep 09 11:15:00 +0000 2018\n",
      "5778 tweets collected for hashtag libspill. Most recent tweeted at Sun Sep 09 05:44:40 +0000 2018\n",
      "5878 tweets collected for hashtag libspill. Most recent tweeted at Sun Sep 09 01:11:26 +0000 2018\n",
      "5978 tweets collected for hashtag libspill. Most recent tweeted at Sat Sep 08 23:37:27 +0000 2018\n",
      "6078 tweets collected for hashtag libspill. Most recent tweeted at Sat Sep 08 22:27:04 +0000 2018\n"
     ]
    }
   ],
   "source": [
    "tweet_collection = collect_tweets(api, hashtag, 100, 6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the colleted tweets to a file, to save on future API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filename = f\"cached_tweets_{hashtag}.pkl\"\n",
    "with open(tweets_filename, 'wb') as f:\n",
    "    pickle.dump(tweet_collection, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Document Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Contents](#Topic-1:-Text-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tweets from the stored .pkl file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_filename = f\"cached_tweets_{hashtag}.pkl\"\n",
    "tweet_collection = None\n",
    "with open(tweets_filename, 'rb') as f:\n",
    "    tweet_collection = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of raw uncleaned tweets is 6078.\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of raw uncleaned tweets is {}.\".format(len(tweet_collection)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a series of transformations in a pipeline to extract documents in the format and with the content we want, from the tweet collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_documents(tweets):\n",
    "    extract_text = lambda tweets: [tweet['full_text'] for tweet in tweets]\n",
    "    convert_whitespace_chars = lambda tweets: [tweet.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ') for tweet in tweets]\n",
    "    squash_whitespace = lambda tweets: [tweet.replace('  ', ' ') for tweet in tweets]\n",
    "    tokenize = lambda tweets: [tweet.strip().split() for tweet in tweets]\n",
    "    strip_links = lambda tweets: [[token for token in tokens if \"http\" not in token] for tokens in tweets]\n",
    "    strip_mentions = lambda tweets: [[token for token in tokens if token[0] is not '@'] for tokens in tweets]\n",
    "    strip_hashtags = lambda tweets: [[token for token in tokens if token[0] is not '#'] for tokens in tweets]\n",
    "    filter_empty = lambda tweets: [tweet for tweet in tweets if len(tweet) > 0]\n",
    "    filter_retweets = lambda tweets: [tweet for tweet in tweets if tweet[0] != 'RT']\n",
    "    rejoin = lambda tweets: [' '.join(tokens) for tokens in tweets]\n",
    "    filter_short = lambda tweets: [tweet for tweet in tweets if len(tweet) > 40]\n",
    "    \n",
    "    documents = tweets\n",
    "    \n",
    "    for transformation in [\n",
    "        extract_text,\n",
    "        convert_whitespace_chars,\n",
    "        squash_whitespace,\n",
    "        tokenize,\n",
    "        strip_links,\n",
    "        strip_mentions,\n",
    "        strip_hashtags,\n",
    "        filter_empty,\n",
    "        filter_retweets,\n",
    "        rejoin,\n",
    "        filter_short,\n",
    "    ]:\n",
    "        documents = transformation(documents)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = extract_documents(tweet_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter out a number of tweets (particularly retweets, since they're duplicate content), so let's see how many documents we have now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of cleaned documents is 941.\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of cleaned documents is {}.\".format(len(documents)))\n",
    "\n",
    "# for viewing troublesome documents Watson can't handle...\n",
    "\n",
    "#for doc in documents:\n",
    "#    if documents.index(doc) + 1 >= 30 and documents.index(doc) + 1 <= 40:\n",
    "#        print(doc + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Watson NLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Contents](#Topic-1:-Text-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an NLU instance using IBM Watson SDK. Created using Sydney as the optional location selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://gateway-syd.watsonplatform.net/natural-language-understanding/api\"\n",
    "version = \"2018-03-19\"\n",
    "\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version=version,\n",
    "    iam_apikey=WATSON_NLU_API_KEY,\n",
    "    url=url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function which retrieves Watson NLU analyses for each of our documents using the NLU instance, and tracks progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analyses(documents):\n",
    "    entities = EntitiesOptions(sentiment=True, emotion=True, limit=5)\n",
    "    sentiment = SentimentOptions()\n",
    "    categories = CategoriesOptions()\n",
    "    keywords = KeywordsOptions(sentiment=True, emotion=True, limit=5)\n",
    "    emotion = EmotionOptions()\n",
    "    document_count = 0\n",
    "    \n",
    "    def analyze(document):\n",
    "        count = documents.index(document) + 1\n",
    "        if count % 10 == 0 or count == len(documents):\n",
    "            print(\"Analysing document #{} of {}...\".format(count, len(documents)))\n",
    "        return natural_language_understanding.analyze(\n",
    "            text=document,\n",
    "            features=Features(\n",
    "                entities=entities,\n",
    "                sentiment=sentiment,\n",
    "                categories=categories,\n",
    "                keywords=keywords,\n",
    "                emotion=emotion,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return [analyze(document) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing document #10 of 941...\n",
      "Analysing document #20 of 941...\n",
      "Analysing document #30 of 941...\n",
      "Analysing document #40 of 941...\n",
      "Analysing document #50 of 941...\n",
      "Analysing document #60 of 941...\n",
      "Analysing document #70 of 941...\n",
      "Analysing document #80 of 941...\n",
      "Analysing document #90 of 941...\n",
      "Analysing document #40 of 941...\n",
      "Analysing document #100 of 941...\n",
      "Analysing document #110 of 941...\n",
      "Analysing document #120 of 941...\n",
      "Analysing document #130 of 941...\n",
      "Analysing document #40 of 941...\n",
      "Analysing document #160 of 941...\n",
      "Analysing document #170 of 941...\n",
      "Analysing document #180 of 941...\n",
      "Analysing document #190 of 941...\n",
      "Analysing document #200 of 941...\n",
      "Analysing document #210 of 941...\n",
      "Analysing document #230 of 941...\n",
      "Analysing document #240 of 941...\n",
      "Analysing document #250 of 941...\n",
      "Analysing document #40 of 941...\n",
      "Analysing document #280 of 941...\n",
      "Analysing document #290 of 941...\n",
      "Analysing document #300 of 941...\n",
      "Analysing document #310 of 941...\n",
      "Analysing document #330 of 941...\n",
      "Analysing document #340 of 941...\n",
      "Analysing document #350 of 941...\n",
      "Analysing document #360 of 941...\n",
      "Analysing document #370 of 941...\n",
      "Analysing document #380 of 941...\n",
      "Analysing document #390 of 941...\n",
      "Analysing document #400 of 941...\n",
      "Analysing document #40 of 941...\n",
      "Analysing document #420 of 941...\n",
      "Analysing document #430 of 941...\n",
      "Analysing document #440 of 941...\n",
      "Analysing document #460 of 941...\n",
      "Analysing document #470 of 941...\n",
      "Analysing document #480 of 941...\n",
      "Analysing document #490 of 941...\n",
      "Analysing document #500 of 941...\n",
      "Analysing document #510 of 941...\n",
      "Analysing document #40 of 941...\n",
      "Analysing document #520 of 941...\n",
      "Analysing document #530 of 941...\n",
      "Analysing document #540 of 941...\n",
      "Analysing document #550 of 941...\n",
      "Analysing document #560 of 941...\n",
      "Analysing document #570 of 941...\n",
      "Analysing document #580 of 941...\n",
      "Analysing document #590 of 941...\n",
      "Analysing document #570 of 941...\n",
      "Analysing document #610 of 941...\n",
      "Analysing document #40 of 941...\n",
      "Analysing document #570 of 941...\n",
      "Analysing document #570 of 941...\n",
      "Analysing document #640 of 941...\n",
      "Analysing document #650 of 941...\n",
      "Analysing document #660 of 941...\n",
      "Analysing document #670 of 941...\n",
      "Analysing document #680 of 941...\n",
      "Analysing document #690 of 941...\n",
      "Analysing document #700 of 941...\n",
      "Analysing document #710 of 941...\n",
      "Analysing document #720 of 941...\n",
      "Analysing document #750 of 941...\n",
      "Analysing document #790 of 941...\n",
      "Analysing document #800 of 941...\n",
      "Analysing document #820 of 941...\n",
      "Analysing document #830 of 941...\n",
      "Analysing document #840 of 941...\n",
      "Analysing document #850 of 941...\n",
      "Analysing document #860 of 941...\n",
      "Analysing document #870 of 941...\n",
      "Analysing document #880 of 941...\n",
      "Analysing document #890 of 941...\n",
      "Analysing document #900 of 941...\n",
      "Analysing document #910 of 941...\n",
      "Analysing document #920 of 941...\n",
      "Analysing document #930 of 941...\n",
      "Analysing document #940 of 941...\n",
      "Analysing document #941 of 941...\n"
     ]
    }
   ],
   "source": [
    "watson_analyses = get_analyses(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out analyses to local disk, so we can perform analysis without refetching each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "watson_analysis_filename = f\"cached_watson_analysis_{hashtag}.pkl\"\n",
    "\n",
    "with open(watson_analysis_filename, 'wb') as f:\n",
    "    pickle.dump(watson_analyses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Analysis of Watson Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Contents](#Topic-1:-Text-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in our Watson analyses from disk, build five analysis objects with the information we need for our report, and write those objects out to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "watson_analyses = None\n",
    "with open(watson_analysis_filename, 'rb') as f:\n",
    "    watson_analyses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_anaylsis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'negative': 560, 'neutral': 216, 'positive': 165})\n"
     ]
    }
   ],
   "source": [
    "sentiment_labels = [analysis.result['sentiment']['document']['label'] for analysis in watson_analyses]\n",
    "sentiment_counter = Counter(sentiment_labels)\n",
    "print(sentiment_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_anaylsis['positive_percentage'] = 100 * sentiment_counter['positive'] / sum(sentiment_counter.values())\n",
    "sentiment_anaylsis['neutral_percentage'] = 100 * sentiment_counter['neutral'] / sum(sentiment_counter.values())\n",
    "sentiment_anaylsis['negative_percentage'] = 100 * sentiment_counter['negative'] / sum(sentiment_counter.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of positive documents is: \t17.53%\n",
      "The percentage of negative documents is: \t59.51%\n",
      "The percentage of neutral documents is: \t22.95%\n"
     ]
    }
   ],
   "source": [
    "print(\"The percentage of positive documents is: \\t{:.2f}%\".format(sentiment_anaylsis['positive_percentage']))\n",
    "print(\"The percentage of negative documents is: \\t{:.2f}%\".format(sentiment_anaylsis['negative_percentage']))\n",
    "print(\"The percentage of neutral documents is: \\t{:.2f}%\".format(sentiment_anaylsis['neutral_percentage']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentiment_scores = []\n",
    "neg_sentiment_scores = []\n",
    "neu_sentiment_scores = []\n",
    "for analysis in watson_analyses:\n",
    "    if analysis.result['sentiment']['document']['label'] == \"positive\":\n",
    "        pos_sentiment_scores.append(analysis.result['sentiment']['document']['score'])\n",
    "    elif analysis.result['sentiment']['document']['label'] == \"negative\":\n",
    "        neg_sentiment_scores.append(analysis.result['sentiment']['document']['score'])\n",
    "    elif analysis.result['sentiment']['document']['label'] == \"neutral\":\n",
    "        neu_sentiment_scores.append(analysis.result['sentiment']['document']['score'])\n",
    "#print(len(pos_sentiment_scores))\n",
    "#print(len(neg_sentiment_scores))\n",
    "#print(len(neu_sentiment_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_anaylsis['average_pos_score'] = sum(pos_sentiment_scores) / len(pos_sentiment_scores)\n",
    "sentiment_anaylsis['std_dev_pos_score'] = statistics.stdev(pos_sentiment_scores)\n",
    "sentiment_anaylsis['average_neg_score'] = sum(neg_sentiment_scores) / len(neg_sentiment_scores)\n",
    "sentiment_anaylsis['std_dev_neg_score'] = statistics.stdev(neg_sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average of the positive sentiment scores is: \t0.571\t(Std. Dev. = 0.288)\n",
      "The average of the negative sentiment scores is: \t-0.618\t(Std. Dev. = 0.204)\n"
     ]
    }
   ],
   "source": [
    "print(\"The average of the positive sentiment scores is: \\t{:.3f}\\t(Std. Dev. = {:.3f})\".format(\n",
    "    sentiment_anaylsis['average_pos_score'], sentiment_anaylsis['std_dev_pos_score']))\n",
    "print(\"The average of the negative sentiment scores is: \\t{:.3f}\\t(Std. Dev. = {:.3f})\".format(\n",
    "    sentiment_anaylsis['average_neg_score'], sentiment_anaylsis['std_dev_neg_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emotion  Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_analysis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document level emotion scores\n",
    "for emotion in ['sadness', 'joy', 'fear', 'disgust', 'anger']:\n",
    "    emotion_scores = [analysis.result['emotion']['document']['emotion'][emotion] for analysis in watson_analyses]\n",
    "\n",
    "    average_emotion_score = sum(emotion_scores) / len(emotion_scores)\n",
    "    emotion_score_std_dev = statistics.stdev(emotion_scores)\n",
    "    emotion_analysis[emotion] = {'average_score': average_emotion_score, 'score_std_dev': emotion_score_std_dev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average score for 'sadness' is: \t0.292\t(Std. Dev. = 0.178)\n",
      "The average score for 'joy' is: \t0.194\t(Std. Dev. = 0.205)\n",
      "The average score for 'fear' is: \t0.140\t(Std. Dev. = 0.113)\n",
      "The average score for 'disgust' is: \t0.240\t(Std. Dev. = 0.189)\n",
      "The average score for 'anger' is: \t0.225\t(Std. Dev. = 0.159)\n"
     ]
    }
   ],
   "source": [
    "for emotion in emotion_analysis.keys():\n",
    "    print(\"The average score for '{}' is: \\t{:.3f}\\t(Std. Dev. = {:.3f})\".format(\n",
    "        emotion, emotion_analysis[emotion]['average_score'], emotion_analysis[emotion]['score_std_dev']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_analysis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses_categories = [analysis.result['categories'] for analysis in watson_analyses]\n",
    "labels = lambda categories: [category['label'] for category in categories]\n",
    "category_labels = []\n",
    "[category_labels.extend(labels(categories)) for categories in analyses_categories]\n",
    "categories_counter = Counter(category_labels)\n",
    "category_analysis['count'] = sum(categories_counter.values())\n",
    "category_analysis['categories'] = dict(categories_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of categories in the corpus is: \t2766\n",
      "\n",
      "The top 50 most common categories as follows: \n",
      "\n",
      "/law, govt and politics/government  275\n",
      "/law, govt and politics/government/parliament  218\n",
      "/travel/tourist destinations/australia and new zealand  91\n",
      "/law, govt and politics/immigration  90\n",
      "/news  86\n",
      "/law, govt and politics/politics/elections  74\n",
      "/art and entertainment/humor  65\n",
      "/business and industrial  53\n",
      "/art and entertainment/movies and tv/movies  50\n",
      "/society/unrest and war  49\n",
      "/style and fashion/clothing/shorts  47\n",
      "/law, govt and politics/politics  46\n",
      "/law, govt and politics  45\n",
      "/family and parenting/children  43\n",
      "/society/work/unions  40\n",
      "/education/school  33\n",
      "/food and drink  30\n",
      "/law, govt and politics/politics/elections/presidential elections  28\n",
      "/society/sex  27\n",
      "/law, govt and politics/legal issues/legislation  24\n",
      "/law, govt and politics/politics/political parties  22\n",
      "/art and entertainment/music  21\n",
      "/law, govt and politics/law enforcement/police  18\n",
      "/travel/tourist destinations/canada  16\n",
      "/art and entertainment/books and literature  16\n",
      "/shopping/toys/puppets  16\n",
      "/business and industrial/energy/oil  15\n",
      "/sports/running and jogging  15\n",
      "/society/crime/personal offense/homicide  15\n",
      "/hobbies and interests/guitar  15\n",
      "/society/social institution/marriage  14\n",
      "/society/racism  13\n",
      "/technology and computing/consumer electronics/audio equipment/speakers  13\n",
      "/sports/hockey  13\n",
      "/health and fitness  13\n",
      "/law, govt and politics/government/courts and judiciary  13\n",
      "/society  12\n",
      "/art and entertainment/theatre  12\n",
      "/science/mathematics/statistics  11\n",
      "/art and entertainment/movies and tv/television  11\n",
      "/business and industrial/energy/oil/oil and gas prices  11\n",
      "/religion and spirituality/christianity  11\n",
      "/science/social science/history  10\n",
      "/art and entertainment/shows and events/sports event  10\n",
      "/society/crime  10\n",
      "/society/dating  10\n",
      "/art and entertainment/dance  9\n",
      "/food and drink/beverages/non alcoholic beverages/coffee and tea  9\n",
      "/shopping/gifts  9\n",
      "/health and fitness/disease  9\n"
     ]
    }
   ],
   "source": [
    "print(\"The total number of categories in the corpus is: \\t{}\\n\".format(category_analysis['count']))\n",
    "print(\"The top 50 most common categories as follows: \\n\")\n",
    "\n",
    "for category, count in categories_counter.most_common(50):\n",
    "    print(\"{}  {}\".format(category, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_analysis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses_entities = [analysis.result['entities'] for analysis in analyses]\n",
    "raw_entities = lambda entities: [entity for entity in entities]\n",
    "entities = []\n",
    "[entities.extend(raw_entities(analyses_entity)) for analyses_entity in analyses_entities]\n",
    "\n",
    "unique_entity_names = set([entity['text'] for entity in entities])\n",
    "\n",
    "entity_analysis['count'] = len(unique_entity_names)\n",
    "entity_analysis['entities'] = {}\n",
    "\n",
    "for entity_name in unique_entity_names:\n",
    "    sentiments = []\n",
    "    for entity in entities:\n",
    "        if entity['text'] == entity_name:\n",
    "            sentiments.append(entity['sentiment']['score'])\n",
    "    entity_analysis['entities'][entity_name] = {}\n",
    "    entity_analysis['entities'][entity_name]['frequency'] = len(sentiments)\n",
    "    entity_analysis['entities'][entity_name]['average_sentiment'] = sum(sentiments) / len(sentiments)\n",
    "    if (len(sentiments) > 1):\n",
    "        entity_analysis['entities'][entity_name]['sentiment_std_dev'] = statistics.stdev(sentiments)\n",
    "    else:\n",
    "        entity_analysis['entities'][entity_name]['sentiment_std_dev'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_analysis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyses_keywords = [analysis.result['keywords'] for analysis in analyses]\n",
    "raw_keywords = lambda keywords: [keyword for keyword in keywords]\n",
    "keywords = []\n",
    "[keywords.extend(raw_keywords(analyses_keyword)) for analyses_keyword in analyses_keywords]\n",
    "\n",
    "unique_keyword_names = set([keyword['text'] for keyword in keywords])\n",
    "\n",
    "keyword_analysis['count'] = len(unique_keyword_names)\n",
    "keyword_analysis['keywords'] = {}\n",
    "\n",
    "for keyword_name in unique_keyword_names:\n",
    "    sentiments = []\n",
    "    for keyword in keywords:\n",
    "        if keyword['text'] == keyword_name:\n",
    "            sentiments.append(keyword['sentiment']['score'])\n",
    "    keyword_analysis['keywords'][keyword_name] = {}\n",
    "    keyword_analysis['keywords'][keyword_name]['frequency'] = len(sentiments)\n",
    "    keyword_analysis['keywords'][keyword_name]['average_sentiment'] = sum(sentiments) / len(sentiments)\n",
    "    if (len(sentiments) > 1):\n",
    "        keyword_analysis['keywords'][keyword_name]['sentiment_std_dev'] = statistics.stdev(sentiments)\n",
    "    else:\n",
    "        keyword_analysis['keywords'][keyword_name]['sentiment_std_dev'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write our analyses out to disk, so they can be used in the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_analyses = {\n",
    "    'sentiment': sentiment_anaylsis,\n",
    "    'emotion': emotion_analysis,\n",
    "    'category': category_analysis,\n",
    "    'keyword': keyword_analysis,\n",
    "    'entity': entity_analysis,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_analysis_filename = f\"cached_report_analysis_{hashtags[0]}.pkl\"\n",
    "\n",
    "with open(report_analysis_filename, 'wb') as f:\n",
    "    pickle.dump(report_analyses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Contents](#Topic-1:-Text-Analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
