{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and filtering tweets\n",
    "\n",
    "The extract_documents method is quite large, but it's convenient to contain all our transformations in a pipeline\n",
    "to add an additional transformation, create a lambda and add it into the right point in the pipe\n",
    "\n",
    "Note that this compositional approach allows us to do convenient things, like filtering out empty\n",
    "tweets more than once, so other transformations don't blow up or need extra logic\n",
    "    \n",
    "### TODO\n",
    "\n",
    "1. other things we may want to exclude\n",
    "    - emoji tokens\n",
    "    - tweets that are too short\n",
    "2. ...\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "Looks like we might need to make sure we're using the right api endpoint, mode, version etc, so that we get the full version of the tweets with full_text/text longer than 140 chars.\n",
    "\n",
    "```\n",
    "t = tweets[4]\n",
    "print(t.text)\n",
    "# => RT @Boeufblogginon: @margokingston1 @cunningham_cch @Bowenchris @quaedvliegs Dutton is one nasty piece of work. Could you imagine what he wâ€¦\n",
    "\n",
    "print(t.full_text)\n",
    "# => None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_documents(tweets):\n",
    "    extract_text = lambda tweets: [tweet.text for tweet in tweets]\n",
    "    convert_whitespace_chars = lambda tweets: [tweet.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ') for tweet in tweets]\n",
    "    squash_whitespace = lambda tweets: [tweet.replace('  ', ' ') for tweet in tweets]\n",
    "    tokenize = lambda tweets: [tweet.strip().split() for tweet in tweets]\n",
    "    strip_links = lambda tweets: [[token for token in tokens if \"http\" not in token] for tokens in tweets]\n",
    "    strip_mentions = lambda tweets: [[token for token in tokens if token[0] is not '@'] for tokens in tweets]\n",
    "    strip_hashtags = lambda tweets: [[token for token in tokens if token[0] is not '#'] for tokens in tweets]\n",
    "    filter_empty = lambda tweets: [tweet for tweet in tweets if len(tweet) > 0]\n",
    "    filter_retweets = lambda tweets: [tweet for tweet in tweets if tweet[0] != 'RT']\n",
    "    rejoin = lambda tweets: [' '.join(tokens) for tokens in tweets]\n",
    "\n",
    "    documents = tweets\n",
    "    \n",
    "    for transformation in [\n",
    "        extract_text,\n",
    "        convert_whitespace_chars,\n",
    "        squash_whitespace,\n",
    "        tokenize,\n",
    "        strip_links,\n",
    "        strip_mentions,\n",
    "        strip_hashtags,\n",
    "        filter_empty,\n",
    "        filter_retweets,\n",
    "        filter_empty,\n",
    "        rejoin,\n",
    "    ]:\n",
    "        documents = transformation(documents)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration: applying our pipeline to locally stored data\n",
    "import pickle\n",
    "import twitter\n",
    "\n",
    "term = \"libspill\"\n",
    "cache_filename = f\"cached_tweets_{term}.pkl\"\n",
    "tweets = None\n",
    "with open(cache_filename, 'rb') as f:\n",
    "    tweets = pickle.load(f)\n",
    "documents = extract_documents(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: we could use this approach for streaming, but probably we will just read all of them from the file\n",
    "def append_tweets(documents, tweets):\n",
    "    new_documents = extract_documents(tweets)\n",
    "    \n",
    "    documents.extend(new_documents)\n",
    "\n",
    "    documents = list(set(documents))\n",
    "\n",
    "# eg:\n",
    "#documents = []\n",
    "#append_tweets(documents, tweets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
